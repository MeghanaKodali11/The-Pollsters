{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now is the time for states to ensure the voices of the voters are being heard. \n",
      "We can all help reduce spreading #COVID19 by taking simple, effective precautions. Listen to the CDC for more tips! https://t.co/wLhwmL7qkj\n",
      "RT @DNCWarRoom: TRUMP: It's the \"story of life\" that wealthy and well-connected people can get coronavirus tests before those who need it.\n",
      "RT @HouseDemocrats: Now more than ever, we must remember that we are ALL in this together.   Please:  ðŸ§¼Wash your hands.\n",
      "RT @BarackObama: We owe a profound debt of gratitude to all our health professionals and everybody whoâ€™ll be on the front lines of thi.\n",
      "Protect yourself and others from Corona\n"
     ]
    }
   ],
   "source": [
    "f = open('test.txt','r')\n",
    "message = f.read()\n",
    "print(message)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = 'test.txt'\n",
    "splittingtxt = '\\n'\n",
    "filenameformat = '\\\\Users\\\\magik\\\\Documents\\\\Advanced Machine Learning\\\\Project\\\\Splitting files\\\\file#.txt'\n",
    "\n",
    "def output(filenum,line):\n",
    "    filename = filenameformat.replace('#',str(filenum) )\n",
    "    fout=open(filename,'w')\n",
    "    fout.write(line)\n",
    "    fout.close\n",
    "\n",
    "file = open( inputfile )\n",
    "lines=file.read().split(splittingtxt)\n",
    "k=0\n",
    "for i in range(0,len( lines ) ):\n",
    "    retweet = lines[i].startswith('RT')\n",
    "    \n",
    "    if retweet == False:\n",
    "        output(k+1, lines[i] )\n",
    "        k=k+1\n",
    "        \n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_txt_files =[]\n",
    "for file in Path(\"\\\\Users\\\\magik\\\\Documents\\\\Advanced Machine Learning\\\\Project\\\\Splitting files\").rglob(\"*.txt\"):\n",
    "     all_txt_files.append(file.parent / file.name)\n",
    "# counts the length of the list\n",
    "n_files = len(all_txt_files)\n",
    "print(n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for txt_file in all_txt_files:\n",
    "    with open(txt_file) as f:\n",
    "        txt_file_as_string = f.read()\n",
    "    all_docs.append(txt_file_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the TfidfVectorizer from Scikit-Learn.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make the output folder if it doesn't already exist\n",
    "Path(\"\\\\Users\\\\magik\\\\Documents\\\\Advanced Machine Learning\\\\Project\\\\Splitting files\\\\Output\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output\n",
    "output_filenames = [str(txt_file).replace(\".txt\", \".csv\").replace(\"\\\\Users\\\\magik\\\\Documents\\\\Advanced Machine Learning\\\\Project\\\\Splitting files\", \"\\\\Users\\\\magik\\\\Documents\\\\Advanced Machine Learning\\\\Project\\\\Splitting files\\\\Output\") for txt_file in all_txt_files]\n",
    "\n",
    "# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    one_doc_as_df.to_csv(output_filenames[counter])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'and',\n",
       " 'are',\n",
       " 'being',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cdc',\n",
       " 'co',\n",
       " 'corona',\n",
       " 'covid19',\n",
       " 'effective',\n",
       " 'ensure',\n",
       " 'from',\n",
       " 'heard',\n",
       " 'help',\n",
       " 'https',\n",
       " 'is',\n",
       " 'listen',\n",
       " 'more',\n",
       " 'now',\n",
       " 'of',\n",
       " 'others',\n",
       " 'precautions',\n",
       " 'protect',\n",
       " 'reduce',\n",
       " 'simple',\n",
       " 'spreading',\n",
       " 'states',\n",
       " 'taking',\n",
       " 'time',\n",
       " 'tips',\n",
       " 'voices',\n",
       " 'voters',\n",
       " 'we',\n",
       " 'wlhwml7qkj',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>being</th>\n",
       "      <th>by</th>\n",
       "      <th>can</th>\n",
       "      <th>cdc</th>\n",
       "      <th>co</th>\n",
       "      <th>corona</th>\n",
       "      <th>covid19</th>\n",
       "      <th>...</th>\n",
       "      <th>spreading</th>\n",
       "      <th>states</th>\n",
       "      <th>taking</th>\n",
       "      <th>time</th>\n",
       "      <th>tips</th>\n",
       "      <th>voices</th>\n",
       "      <th>voters</th>\n",
       "      <th>we</th>\n",
       "      <th>wlhwml7qkj</th>\n",
       "      <th>yourself</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        all       and       are     being        by       can       cdc  \\\n",
       "0  0.000000  0.000000  1.693147  1.693147  0.000000  0.000000  0.000000   \n",
       "1  1.693147  0.000000  0.000000  0.000000  1.693147  1.693147  1.693147   \n",
       "2  0.000000  1.693147  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         co    corona   covid19  ...  spreading    states    taking      time  \\\n",
       "0  0.000000  0.000000  0.000000  ...   0.000000  1.693147  0.000000  1.693147   \n",
       "1  1.693147  0.000000  1.693147  ...   1.693147  0.000000  1.693147  0.000000   \n",
       "2  0.000000  1.693147  0.000000  ...   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       tips    voices    voters        we  wlhwml7qkj  yourself  \n",
       "0  0.000000  1.693147  1.693147  0.000000    0.000000  0.000000  \n",
       "1  1.693147  0.000000  0.000000  1.693147    1.693147  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000    0.000000  1.693147  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transformed_documents_as_array, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
